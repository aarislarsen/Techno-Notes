# PDF LLM Processor

> üöÄ Automated PDF document analysis using local LLM (Ollama). Zero-configuration setup for Ubuntu/WSL.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)

## ‚ú® Features

- üîí **Privacy-First**: All processing happens locally - your data never leaves your machine
- üöÄ **Fully Automated Setup**: One-click installation of all dependencies including Ollama
- üéØ **Simple Interface**: Beautiful drag-and-drop web UI with customizable prompts
- üõ°Ô∏è **Security Hardened**: Input validation, rate limiting, sanitization, and secure file handling
- üì¶ **Self-Contained**: Isolated virtual environment with dependency management
- ‚ö° **Multiple Models**: Support for Llama, Mistral, Phi, and more

## üöÄ Quick Start

### Prerequisites

- Ubuntu 20.04+ or WSL2 with Ubuntu
- 8GB RAM minimum (16GB recommended)
- 10GB free disk space
- Internet connection (for initial setup)

### Installation
```bash
# Clone the repository
git clone https://github.com/yourusername/pdf-llm-processor.git
cd pdf-llm-processor

# Run automated setup
chmod +x setup.sh
./setup.sh
```

The setup script automatically:
- ‚úÖ Installs Python 3 and system dependencies
- ‚úÖ Creates isolated virtual environment
- ‚úÖ Installs Python packages (Flask, PyPDF2, requests)
- ‚úÖ Sets up application structure with proper permissions

### First Run
```bash
./run.sh
```

Open your browser to: **http://localhost:5000**

Click **"Start Automatic Setup"** to:
1. Install Ollama automatically
2. Download your selected LLM model (choose smaller models like phi or llama3.2:1b for faster setup)
3. Start the service

‚è±Ô∏è **Note**: Initial setup downloads 1-8GB of model data depending on model choice.

### Subsequent Runs
```bash
./run.sh
```

System starts immediately with no setup required.

## üìñ Usage

### Basic Workflow

1. **Configure Analysis Prompt**
   - Edit the prompt in the web interface to customize how the AI analyzes documents
   - Default prompt provides comprehensive summaries with key points

2. **Upload PDF**
   - Drag and drop PDF file (max 50MB, 100 pages)
   - Or click to browse and select file

3. **Wait for Processing**
   - Processing time varies by model size and document length
   - Typically 30 seconds to 3 minutes

4. **Download Results**
   - Analysis automatically downloads as a text file
   - Original filename preserved with "_output.txt" suffix

### Custom Prompts

Edit the prompt to analyze documents for specific purposes:

**Executive Summary:**
```
Provide an executive summary of this document including:
- Key business objectives
- Main findings
- Financial implications
- Recommended actions
```

**Technical Analysis:**
```
Analyze the technical aspects of this document:
- Technical specifications
- Implementation details
- Potential challenges
- Technology stack recommendations
```

**Legal Review:**
```
Review this document focusing on:
- Key contractual obligations
- Risk factors
- Compliance requirements
- Recommended legal actions
```

## ü§ñ Supported Models

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| llama3.2:1b | 1B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê | Ultra-fast, testing |
| phi | 2.7B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | Quick analysis |
| llama3.2:3b | 3B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | Balanced speed/quality |
| llama2 | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | **Recommended default** |
| mistral | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Efficient processing |
| codellama | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Code-heavy documents |
| llama2:13b | 13B | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | High quality analysis |

## üèóÔ∏è Architecture
```
pdf-llm-processor/
‚îú‚îÄ‚îÄ app.py                 # Flask application with security hardening
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html        # Modern responsive web interface
‚îú‚îÄ‚îÄ uploads/              # Temporary upload storage (auto-cleanup)
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ outputs/              # Generated analysis files (auto-cleanup)
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ logs/                 # Application logs
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ prompt.txt            # Customizable analysis prompt
‚îú‚îÄ‚îÄ llm_config.json       # Configuration (auto-generated)
‚îú‚îÄ‚îÄ venv/                 # Isolated Python environment
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies
‚îú‚îÄ‚îÄ setup.sh              # Automated setup script
‚îú‚îÄ‚îÄ run.sh                # Application launcher
‚îú‚îÄ‚îÄ .gitignore           # Git ignore rules
‚îú‚îÄ‚îÄ LICENSE              # MIT License
‚îî‚îÄ‚îÄ README.md            # This file
```

## üîí Security Features

### Input Validation
- ‚úÖ Filename sanitization and path traversal prevention
- ‚úÖ Model name whitelist validation
- ‚úÖ File size limits (50MB max)
- ‚úÖ File type verification (PDF only)
- ‚úÖ Text length limits at multiple checkpoints

### Data Protection
- ‚úÖ All processing happens locally (no external API calls)
- ‚úÖ Automatic file cleanup (1-hour retention)
- ‚úÖ Restrictive file permissions (0600)
- ‚úÖ Secure session management
- ‚úÖ XSS prevention with HTML escaping

### Network Security
- ‚úÖ Ollama service bound to localhost only (127.0.0.1)
- ‚úÖ Flask application configurable binding
- ‚úÖ Rate limiting (10 requests/minute per IP)
- ‚úÖ Request timeout enforcement

### Application Security
- ‚úÖ No shell injection vulnerabilities
- ‚úÖ Comprehensive error logging
- ‚úÖ Graceful shutdown handling
- ‚úÖ Process isolation
- ‚úÖ Input sanitization on all endpoints

## üêõ Troubleshooting

### Port Already in Use
```bash
# Find process using port 5000
sudo lsof -i :5000

# Kill the process
sudo kill -9 <PID>
```

### Ollama Service Won't Start
```bash
# Check if Ollama is installed
ollama --version

# Manually start Ollama
ollama serve

# Verify it's running
curl http://localhost:11434/api/tags
```

### Model Download Fails
```bash
# Check disk space
df -h

# Manually download model
ollama pull llama2

# Verify download
ollama list
```

### WSL Network Issues

**Can't access from Windows browser:**

1. Get WSL IP address:
```bash
   ip addr show eth0 | grep inet
```

2. Access via: `http://<WSL_IP>:5000`

**Or** use `localhost:5000` (usually works by default in WSL2).

### Out of Memory

**Solutions:**
- Use smaller model (phi, llama3.2:1b)
- Close other applications
- Increase WSL memory limit (edit `.wslconfig` in Windows user folder)
- Process smaller PDF files

### PDF Text Extraction Fails

**Common causes:**
- Scanned PDFs (image-only, no text layer)
- Encrypted/password-protected PDFs
- Corrupted PDF files

**Solutions:**
- Use OCR to convert scanned PDFs to text-based PDFs
- Remove password protection before upload
- Try re-saving the PDF in a different viewer

## ‚öôÔ∏è Advanced Configuration

### Environment Variables
```bash
# Change Ollama host (if needed)
export OLLAMA_HOST=127.0.0.1:11434
```

### Custom Model Installation
```bash
# Download specific model version
ollama pull llama2:13b

# List available models
ollama list

# Remove unused models to free space
ollama rm <model-name>
```

### Systemd Service (Optional)

Run as a system service that starts automatically:
```bash
# Create service file
sudo nano /etc/systemd/system/pdf-llm-processor.service
```

Add this content (replace YOUR_USERNAME with your actual username):
```ini
[Unit]
Description=PDF LLM Processor
After=network.target

[Service]
Type=simple
User=YOUR_USERNAME
WorkingDirectory=/home/YOUR_USERNAME/pdf-llm-processor
ExecStart=/home/YOUR_USERNAME/pdf-llm-processor/venv/bin/python3 app.py
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Enable and start the service:
```bash
sudo systemctl enable pdf-llm-processor
sudo systemctl start pdf-llm-processor
sudo systemctl status pdf-llm-processor

# View logs
sudo journalctl -u pdf-llm-processor -f
```

### Logging Configuration

Logs are stored in `logs/app.log`. To adjust log level, edit `app.py`:
```python
logging.basicConfig(
    level=logging.DEBUG,  # Change to DEBUG for verbose logging
    # ...
)
```

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Development Setup
```bash
# Clone repository
git clone https://github.com/yourusername/pdf-llm-processor.git
cd pdf-llm-processor

# Run setup
./setup.sh

# Activate virtual environment
source venv/bin/activate

# Make your changes to app.py or other files

# Test your changes
./run.sh
```

### Contribution Guidelines

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Test your changes thoroughly
4. Ensure security best practices are followed
5. Update documentation if needed
6. Commit changes (`git commit -m 'Add amazing feature'`)
7. Push to branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## üìä API Endpoints

For developers who want to integrate programmatically:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Main web interface |
| `/setup_status` | GET | Get system setup status |
| `/auto_setup` | POST | Start automatic Ollama setup |
| `/process_pdf` | POST | Upload and process PDF |
| `/download/<file>` | GET | Download analysis result |
| `/get_prompt` | GET | Get current analysis prompt |
| `/save_prompt` | POST | Save custom analysis prompt |
| `/list_models` | GET | List available models |
| `/set_model` | POST | Change active model |

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [Ollama](https://ollama.ai/) - Local LLM runtime that makes this possible
- [Flask](https://flask.palletsprojects.com/) - Lightweight web framework
- [PyPDF2](https://pypdf2.readthedocs.io/) - PDF processing library
- [Meta AI](https://ai.meta.com/) - Llama models
- [Mistral AI](https://mistral.ai/) - Mistral models

## üìû Support

- üìñ [Documentation](https://github.com/yourusername/pdf-llm-processor/wiki)
- üí¨ [Discussions](https://github.com/yourusername/pdf-llm-processor/discussions)
- üêõ [Issue Tracker](https://github.com/yourusername/pdf-llm-processor/issues)

## üîê Security

Found a security vulnerability? Please email security@yourproject.com instead of opening a public issue.

## üìä Project Status

- ‚úÖ Core functionality complete
- ‚úÖ Security hardening applied
- ‚úÖ Documentation complete
- ‚úÖ Ready for production use
- üîÑ Ongoing maintenance and updates

## üó∫Ô∏è Roadmap

Future planned features:

- [ ] Support for more document formats (DOCX, TXT, Markdown)
- [ ] Batch processing capabilities
- [ ] API-only mode (headless operation)
- [ ] Docker container support
- [ ] Multi-language support for UI
- [ ] Custom model fine-tuning guide
- [ ] Cloud deployment guides (AWS, Azure, GCP)
- [ ] Export to multiple formats (PDF, DOCX, Markdown)

## üí° Use Cases

### Business
- Contract analysis and review
- Report summarization
- Meeting minutes extraction
- Proposal evaluation

### Academic
- Research paper summarization
- Literature review assistance
- Thesis chapter analysis
- Citation extraction

### Legal
- Document discovery
- Contract clause identification
- Compliance checking
- Legal brief analysis

### Technical
- Technical documentation analysis
- API documentation extraction
- Code comment generation from specs
- Architecture decision records

## ‚ö° Performance Tips

1. **Choose the right model**: Smaller models (phi, llama3.2:1b) for quick analysis, larger models (llama2:13b) for quality
2. **Optimize PDFs**: Text-based PDFs process much faster than scanned documents
3. **Limit page count**: The tool processes up to 100 pages; for longer documents, split into sections
4. **Close other applications**: Free up RAM for better performance
5. **Use SSD storage**: Faster model loading and processing

## üîß Customization

### Custom Analysis Templates

Create specialized analysis templates by modifying the prompt. Examples:

**Financial Analysis:**
```
Analyze this financial document and extract:
- Revenue figures and trends
- Cost structures
- Profit margins
- Key financial ratios
- Risk factors mentioned
- Future projections
```

**Medical Records:**
```
Summarize this medical document including:
- Patient symptoms
- Diagnoses
- Treatment plans
- Medications prescribed
- Follow-up recommendations
- Critical findings
```

**Security Consultant Focused:**
```
Analyze this document from a security perspective:
- Identify security requirements
- List vulnerabilities or risks mentioned
- Extract compliance requirements
- Note security controls discussed
- Highlight security recommendations
- Flag any security gaps
```

## üåê Language Support

While the interface is in English, the LLM models support multiple languages for document analysis. You can customize prompts in different languages:

**Spanish:**
```
Analiza este documento PDF y proporciona un resumen completo...
```

**French:**
```
Analysez ce document PDF et fournissez un r√©sum√© complet...
```

**German:**
```
Analysieren Sie dieses PDF-Dokument und erstellen Sie eine umfassende Zusammenfassung...
```

## ‚≠ê Star History

If you find this project useful, please consider giving it a star on GitHub!

## üìà Changelog

### v1.0.0 (2024-01)
- Initial release
- Automated setup for Ubuntu/WSL
- Support for multiple LLM models (Llama, Mistral, Phi)
- Security hardening with input validation and rate limiting
- Comprehensive logging system
- WSL network compatibility
- Beautiful responsive web interface
- Drag-and-drop PDF upload
- Customizable analysis prompts
- Automatic file cleanup
- One-click Ollama installation

---

## üìù FAQ

**Q: Does this work on Windows?**  
A: Yes, via WSL2 (Windows Subsystem for Linux). Native Windows support is not currently available but is on the roadmap.

**Q: Can I use this offline?**  
A: Yes! After initial setup and model download, the entire system works completely offline.

**Q: How much disk space do I need?**  
A: At minimum 10GB: ~2GB for Ollama, 4-8GB for models, rest for system and working space.

**Q: Is my data secure?**  
A: Yes. All processing happens locally on your machine. No data is sent to external servers.

**Q: Can I process multiple PDFs at once?**  
A: Currently one at a time. Batch processing is planned for a future release.

**Q: Which model should I choose?**  
A: For most users, `llama2` (7B) offers the best balance. Use `phi` or `llama3.2:1b` for speed, `llama2:13b` for quality.

**Q: Can I use custom/fine-tuned models?**  
A: Yes! Any model compatible with Ollama can be used. Import it with `ollama pull <model-name>`.

**Q: Does it work on ARM/Apple Silicon?**  
A: Not officially tested, but Ollama supports ARM. May work on Apple Silicon via Rosetta or native ARM build.

---

**Made with ‚ù§Ô∏è for the open-source community**

**Privacy-First | Security-Focused | User-Friendly**
