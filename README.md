# PDF LLM Processor

> ğŸš€ Automated PDF document analysis using local LLM (Ollama). Zero-configuration setup for Ubuntu/WSL.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)

## âœ¨ Features

- ğŸ”’ **Privacy-First**: All processing happens locally - your data never leaves your machine
- ğŸš€ **Fully Automated Setup**: One-click installation of all dependencies including Ollama
- ğŸ¯ **Simple Interface**: Beautiful drag-and-drop web UI with customizable prompts
- ğŸ›¡ï¸ **Security Hardened**: Input validation, rate limiting, sanitization, and secure file handling
- ğŸ“¦ **Self-Contained**: Isolated virtual environment with dependency management
- âš¡ **Multiple Models**: Support for Llama, Mistral, Phi, and more

## ğŸš€ Quick Start

### Prerequisites

- Ubuntu 20.04+ or WSL2 with Ubuntu
- 8GB RAM minimum (16GB recommended)
- 10GB free disk space
- Internet connection (for initial setup)

### Installation
```bash
# Clone the repository
git clone https://github.com/aarislarsen/Techno-Notes.git
cd Techno-Notes

# Run automated setup
chmod +x setup.sh
./setup.sh
```

The setup script automatically:
- âœ… Installs Python 3 and system dependencies
- âœ… Creates isolated virtual environment
- âœ… Installs Python packages (Flask, PyPDF2, requests)
- âœ… Installs Ollama LLM runtime
- âœ… Sets up application structure with proper permissions

### First Run
```bash
./run.sh
```

Open your browser to: **http://localhost:5000**

On first launch, you'll need to:
1. Select your preferred LLM model (choose smaller models like phi or llama3.2:1b for faster setup)
2. Click to download the model
3. Wait for download to complete (1-8GB depending on model choice)

â±ï¸ **Note**: Model download typically takes 2-15 minutes depending on your internet connection and model size.

### Subsequent Runs
```bash
./run.sh
```

System starts immediately with no setup required.

## ğŸ“– Usage

### Basic Workflow

1. **Configure Analysis Prompt**
   - Edit the prompt in the web interface to customize how the AI analyzes documents
   - Default prompt provides comprehensive summaries with key points

2. **Upload PDF**
   - Drag and drop PDF file (max 50MB, 100 pages)
   - Or click to browse and select file

3. **Wait for Processing**
   - Processing time varies by model size and document length
   - Typically 30 seconds to 3 minutes

4. **Download Results**
   - Analysis automatically downloads as a text file
   - Original filename preserved with "_output.txt" suffix

### Custom Prompts

Edit the prompt to analyze documents for specific purposes:

**Executive Summary:**
```
Provide an executive summary of this document including:
- Key business objectives
- Main findings
- Financial implications
- Recommended actions
```

**Technical Analysis:**
```
Analyze the technical aspects of this document:
- Technical specifications
- Implementation details
- Potential challenges
- Technology stack recommendations
```

**Legal Review:**
```
Review this document focusing on:
- Key contractual obligations
- Risk factors
- Compliance requirements
- Recommended legal actions
```

## ğŸ¤– Supported Models

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| llama3.2:1b | 1B | âš¡âš¡âš¡âš¡ | â­ | Ultra-fast, testing |
| phi | 2.7B | âš¡âš¡âš¡ | â­â­ | Quick analysis |
| llama3.2:3b | 3B | âš¡âš¡âš¡ | â­â­ | Balanced speed/quality |
| llama2 | 7B | âš¡âš¡ | â­â­â­ | **Recommended default** |
| mistral | 7B | âš¡âš¡ | â­â­â­ | Efficient processing |
| codellama | 7B | âš¡âš¡ | â­â­â­ | Code-heavy documents |
| llama2:13b | 13B | âš¡ | â­â­â­â­ | High quality analysis |

## ğŸ—ï¸ Architecture
```
Techno-Notes/
â”œâ”€â”€ app.py                 # Flask application with security hardening
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html        # Modern responsive web interface
â”‚   â””â”€â”€ prompt.txt        # Customizable analysis prompt
â”œâ”€â”€ uploads/              # Temporary upload storage (auto-cleanup)
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ outputs/              # Generated analysis files (auto-cleanup)
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ logs/                 # Application logs
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ llm_config.json       # Configuration (auto-generated)
â”œâ”€â”€ venv/                 # Isolated Python environment
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ setup.sh              # Automated setup script
â”œâ”€â”€ run.sh                # Application launcher
â”œâ”€â”€ .gitignore           # Git ignore rules
â”œâ”€â”€ LICENSE              # MIT License
â””â”€â”€ README.md            # This file
```

## ğŸ”’ Security Features

### Input Validation
- âœ… Filename sanitization and path traversal prevention
- âœ… Model name whitelist validation
- âœ… File size limits (50MB max)
- âœ… File type verification (PDF only)
- âœ… Text length limits at multiple checkpoints

### Data Protection
- âœ… All processing happens locally (no external API calls)
- âœ… Automatic file cleanup (1-hour retention)
- âœ… Restrictive file permissions (0600)
- âœ… Secure session management
- âœ… XSS prevention with HTML escaping

### Network Security
- âœ… Ollama service bound to localhost only (127.0.0.1)
- âœ… Flask application configurable binding
- âœ… Rate limiting (10 requests/minute per IP)
- âœ… Request timeout enforcement

### Application Security
- âœ… No shell injection vulnerabilities
- âœ… Comprehensive error logging
- âœ… Graceful shutdown handling
- âœ… Process isolation
- âœ… Input sanitization on all endpoints

## ğŸ› Troubleshooting

### Port Already in Use
```bash
# Find process using port 5000
sudo lsof -i :5000

# Kill the process
sudo kill -9 <PID>
```

### Ollama Service Won't Start
```bash
# Check if Ollama is installed
ollama --version

# If not installed, install manually
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama service
ollama serve

# In another terminal, verify it's running
curl http://localhost:11434/api/tags
```

### Model Download Fails
```bash
# Check disk space
df -h

# Manually download model
ollama pull llama2

# Verify download
ollama list
```

### WSL Network Issues

**Can't access from Windows browser:**

1. Get WSL IP address:
```bash
   ip addr show eth0 | grep inet
```

2. Access via: `http://<WSL_IP>:5000`

**Or** use `localhost:5000` (usually works by default in WSL2).

### Out of Memory

**Solutions:**
- Use smaller model (phi, llama3.2:1b)
- Close other applications
- Increase WSL memory limit (edit `.wslconfig` in Windows user folder)
- Process smaller PDF files

### PDF Text Extraction Fails

**Common causes:**
- Scanned PDFs (image-only, no text layer)
- Encrypted/password-protected PDFs
- Corrupted PDF files

**Solutions:**
- Use OCR to convert scanned PDFs to text-based PDFs
- Remove password protection before upload
- Try re-saving the PDF in a different viewer

## âš™ï¸ Advanced Configuration

### Environment Variables
```bash
# Change Ollama host (if needed)
export OLLAMA_HOST=127.0.0.1:11434
```

### Custom Model Installation
```bash
# Download specific model version
ollama pull llama2:13b

# List available models
ollama list

# Remove unused models to free space
ollama rm <model-name>
```

### Systemd Service (Optional)

Run as a system service that starts automatically:
```bash
# Create service file
sudo nano /etc/systemd/system/pdf-llm-processor.service
```

Add this content (replace YOUR_USERNAME with your actual username):
```ini
[Unit]
Description=PDF LLM Processor
After=network.target

[Service]
Type=simple
User=YOUR_USERNAME
WorkingDirectory=/home/YOUR_USERNAME/Techno-Notes
ExecStart=/home/YOUR_USERNAME/Techno-Notes/venv/bin/python3 app.py
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Enable and start the service:
```bash
sudo systemctl enable pdf-llm-processor
sudo systemctl start pdf-llm-processor
sudo systemctl status pdf-llm-processor

# View logs
sudo journalctl -u pdf-llm-processor -f
```

### Logging Configuration

Logs are stored in `logs/app.log`. To adjust log level, edit `app.py`:
```python
logging.basicConfig(
    level=logging.DEBUG,  # Change to DEBUG for verbose logging
    # ...
)
```

## ğŸ“Š API Endpoints

For developers who want to integrate programmatically:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Main web interface |
| `/setup_status` | GET | Get system setup status |
| `/auto_setup` | POST | Start automatic Ollama setup |
| `/process_pdf` | POST | Upload and process PDF |
| `/download/<file>` | GET | Download analysis result |
| `/get_prompt` | GET | Get current analysis prompt |
| `/save_prompt` | POST | Save custom analysis prompt |
| `/list_models` | GET | List available models |
| `/set_model` | POST | Change active model |

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“Š Project Status

- âœ… Core functionality complete
- âœ… Security hardening applied
- âœ… Documentation complete
- âœ… Ready for production use
- ğŸ”„ Ongoing maintenance and updates

## ğŸ’¡ Use Cases

### Business
- Contract analysis and review
- Report summarization
- Meeting minutes extraction
- Proposal evaluation

### Academic
- Research paper summarization
- Literature review assistance
- Thesis chapter analysis
- Citation extraction

### Legal
- Document discovery
- Contract clause identification
- Compliance checking
- Legal brief analysis

### Technical
- Technical documentation analysis
- API documentation extraction
- Code comment generation from specs
- Architecture decision records

## âš¡ Performance Tips

1. **Choose the right model**: Smaller models (phi, llama3.2:1b) for quick analysis, larger models (llama2:13b) for quality
2. **Optimize PDFs**: Text-based PDFs process much faster than scanned documents
3. **Limit page count**: The tool processes up to 100 pages; for longer documents, split into sections
4. **Close other applications**: Free up RAM for better performance
5. **Use SSD storage**: Faster model loading and processing

## ğŸ”§ Customization

### Custom Analysis Templates

Create specialized analysis templates by modifying the prompt. Examples:

**Financial Analysis:**
```
Analyze this financial document and extract:
- Revenue figures and trends
- Cost structures
- Profit margins
- Key financial ratios
- Risk factors mentioned
- Future projections
```

**Medical Records:**
```
Summarize this medical document including:
- Patient symptoms
- Diagnoses
- Treatment plans
- Medications prescribed
- Follow-up recommendations
- Critical findings
```

**Security Consultant Focused:**
```
Analyze this document from a security perspective:
- Identify security requirements
- List vulnerabilities or risks mentioned
- Extract compliance requirements
- Note security controls discussed
- Highlight security recommendations
- Flag any security gaps
```

## ğŸŒ Language Support

While the interface is in English, the LLM models support multiple languages for document analysis. You can customize prompts in different languages:

**Spanish:**
```
Analiza este documento PDF y proporciona un resumen completo...
```

**French:**
```
Analysez ce document PDF et fournissez un rÃ©sumÃ© complet...
```

**German:**
```
Analysieren Sie dieses PDF-Dokument und erstellen Sie eine umfassende Zusammenfassung...
```

## â­ Star History

If you find this project useful, please consider giving it a star on GitHub!
