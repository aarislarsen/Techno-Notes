# Techno-Notes

> üöÄ Automated PDF document analysis using local LLM (Ollama) or cloud APIs (ChatGPT, Perplexity). Zero-configuration setup for Ubuntu/WSL.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)

## ‚ú® Features

- üîí **Privacy-First Option**: Use local Ollama - your data never leaves your machine
- ‚òÅÔ∏è **Cloud API Support**: Optional integration with ChatGPT and Perplexity APIs
- üöÄ **Fully Automated Setup**: One-click installation of all dependencies including Ollama
- üéØ **Simple Interface**: Beautiful drag-and-drop web UI with customizable prompts
- üõ°Ô∏è **Security Hardened**: Input validation, rate limiting, sanitization, and secure file handling
- üì¶ **Self-Contained**: Isolated virtual environment with dependency management
- ‚ö° **Multiple Providers**: Choose between local Ollama or cloud-based APIs

## üöÄ Quick Start

### Prerequisites

- Ubuntu 20.04+ or WSL2 with Ubuntu
- 8GB RAM minimum (16GB recommended)
- 10GB free disk space
- Internet connection (for initial setup)

### Installation
```bash
# Clone the repository
git clone https://github.com/aarislarsen/Techno-Notes.git
cd Techno-Notes

# Run automated setup
chmod +x setup.sh
./setup.sh
```

The setup script automatically:
- ‚úÖ Installs Python 3 and system dependencies
- ‚úÖ Creates isolated virtual environment
- ‚úÖ Installs Python packages (Flask, PyPDF2, requests)
- ‚úÖ Installs Ollama LLM runtime
- ‚úÖ Sets up application structure with proper permissions

### First Run
```bash
./run.sh
```

Open your browser to: **http://localhost:5000**

On first launch, you'll need to:
1. Select your preferred LLM model (choose smaller models like phi or llama3.2:1b for faster setup)
2. Click to download the model
3. Wait for download to complete (1-8GB depending on model choice)

‚è±Ô∏è **Note**: Model download typically takes 2-15 minutes depending on your internet connection and model size.

### Subsequent Runs
```bash
./run.sh
```

System starts immediately with no setup required.

## üìñ Usage

### Basic Workflow

1. **Choose LLM Provider** (Optional)
   - **Local Ollama** (Default): Privacy-first, runs entirely on your machine
   - **ChatGPT**: OpenAI's GPT models via API
   - **Perplexity**: Perplexity AI's models via API

2. **Configure API (if using cloud providers)**
   - Enter your API key in the provider configuration section
   - Select your preferred model
   - Test the connection to ensure it works

3. **Configure Analysis Prompt**
   - Edit the prompt in the web interface to customize how the AI analyzes documents
   - Default prompt provides comprehensive summaries with key points

4. **Upload PDF**
   - Drag and drop PDF file (max 50MB, 100 pages)
   - Or click to browse and select file

5. **Wait for Processing**
   - Processing time varies by model size and document length
   - Typically 30 seconds to 3 minutes

6. **Download Results**
   - Analysis automatically downloads as a text file
   - Original filename preserved with "_output.txt" suffix

### Using Cloud API Providers

#### ChatGPT (OpenAI)

1. Get an API key from [OpenAI Platform](https://platform.openai.com/api-keys)
2. In the web interface, select "ChatGPT (OpenAI API)"
3. Enter your API key and select a model (gpt-4o-mini recommended for cost-effectiveness)
4. Click "Save API Key" and then "Test Connection"
5. Once connected, you can process PDFs using ChatGPT

**Available Models:**
- `gpt-4o-mini` - Fast and cost-effective (Recommended)
- `gpt-4o` - Most capable
- `gpt-4-turbo` - Advanced reasoning
- `gpt-3.5-turbo` - Fastest and cheapest

#### Perplexity AI

1. Get an API key from [Perplexity AI](https://www.perplexity.ai/settings/api)
2. In the web interface, select "Perplexity AI"
3. Enter your API key and select a model
4. Click "Save API Key" and then "Test Connection"
5. Once connected, you can process PDFs using Perplexity

**Available Models:**
- `llama-3.1-sonar-small-128k-online` - Recommended
- `llama-3.1-sonar-large-128k-online` - Higher quality
- `llama-3.1-sonar-huge-128k-online` - Maximum capability

### Custom Prompts

Edit the prompt to analyze documents for specific purposes:

**Executive Summary:**
```
Provide an executive summary of this document including:
- Key business objectives
- Main findings
- Financial implications
- Recommended actions
```

**Technical Analysis:**
```
Analyze the technical aspects of this document:
- Technical specifications
- Implementation details
- Potential challenges
- Technology stack recommendations
```

**Legal Review:**
```
Review this document focusing on:
- Key contractual obligations
- Risk factors
- Compliance requirements
- Recommended legal actions
```

## ü§ñ Supported Models

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| llama3.2:1b | 1B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê | Ultra-fast, testing |
| phi | 2.7B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | Quick analysis |
| llama3.2:3b | 3B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | Balanced speed/quality |
| llama2 | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | **Recommended default** |
| mistral | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Efficient processing |
| codellama | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Code-heavy documents |
| llama2:13b | 13B | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | High quality analysis |

## üèóÔ∏è Architecture
```
Techno-Notes/
‚îú‚îÄ‚îÄ app.py                 # Flask application with security hardening
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ index.html        # Modern responsive web interface
‚îÇ   ‚îî‚îÄ‚îÄ prompt.txt        # Customizable analysis prompt
‚îú‚îÄ‚îÄ uploads/              # Temporary upload storage (auto-cleanup)
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ outputs/              # Generated analysis files (auto-cleanup)
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ logs/                 # Application logs
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ llm_config.json       # LLM configuration (auto-generated)
‚îú‚îÄ‚îÄ .api_keys.json        # API keys storage (gitignored for security)
‚îú‚îÄ‚îÄ venv/                 # Isolated Python environment
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies
‚îú‚îÄ‚îÄ setup.sh              # Automated setup script
‚îú‚îÄ‚îÄ run.sh                # Application launcher
‚îú‚îÄ‚îÄ .gitignore           # Git ignore rules
‚îú‚îÄ‚îÄ LICENSE              # MIT License
‚îî‚îÄ‚îÄ README.md            # This file
```

## üîí Security Features

### Input Validation
- ‚úÖ Filename sanitization and path traversal prevention
- ‚úÖ Model name whitelist validation
- ‚úÖ File size limits (50MB max)
- ‚úÖ File type verification (PDF only)
- ‚úÖ Text length limits at multiple checkpoints

### Data Protection
- ‚úÖ Local processing option (Ollama - no external API calls)
- ‚úÖ API keys stored securely with restricted file permissions (0600)
- ‚úÖ API keys excluded from git via .gitignore
- ‚úÖ Automatic file cleanup (1-hour retention)
- ‚úÖ Secure session management
- ‚úÖ XSS prevention with HTML escaping

### Network Security
- ‚úÖ Ollama service bound to localhost only (127.0.0.1)
- ‚úÖ Flask application configurable binding
- ‚úÖ Rate limiting (10 requests/minute per IP)
- ‚úÖ Request timeout enforcement
- ‚úÖ HTTPS for all API communications (ChatGPT/Perplexity)

### Application Security
- ‚úÖ No shell injection vulnerabilities
- ‚úÖ Comprehensive error logging
- ‚úÖ Graceful shutdown handling
- ‚úÖ Process isolation
- ‚úÖ Input sanitization on all endpoints
- ‚úÖ Provider validation for API requests

## üêõ Troubleshooting

### Port Already in Use
```bash
# Find process using port 5000
sudo lsof -i :5000

# Kill the process
sudo kill -9 <PID>
```

### Ollama Service Won't Start
```bash
# Check if Ollama is installed
ollama --version

# If not installed, install manually
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama service
ollama serve

# In another terminal, verify it's running
curl http://localhost:11434/api/tags
```

### Model Download Fails
```bash
# Check disk space
df -h

# Manually download model
ollama pull llama2

# Verify download
ollama list
```

### WSL Network Issues

**Can't access from Windows browser:**

1. Get WSL IP address:
```bash
   ip addr show eth0 | grep inet
```

2. Access via: `http://<WSL_IP>:5000`

**Or** use `localhost:5000` (usually works by default in WSL2).

### Out of Memory

**Solutions:**
- Use smaller model (phi, llama3.2:1b)
- Close other applications
- Increase WSL memory limit (edit `.wslconfig` in Windows user folder)
- Process smaller PDF files

### PDF Text Extraction Fails

**Common causes:**
- Scanned PDFs (image-only, no text layer)
- Encrypted/password-protected PDFs
- Corrupted PDF files

**Solutions:**
- Use OCR to convert scanned PDFs to text-based PDFs
- Remove password protection before upload
- Try re-saving the PDF in a different viewer

## ‚öôÔ∏è Advanced Configuration

### Environment Variables
```bash
# Change Ollama host (if needed)
export OLLAMA_HOST=127.0.0.1:11434
```

### Custom Model Installation
```bash
# Download specific model version
ollama pull llama2:13b

# List available models
ollama list

# Remove unused models to free space
ollama rm <model-name>
```

### Systemd Service (Optional)

Run as a system service that starts automatically:
```bash
# Create service file
sudo nano /etc/systemd/system/pdf-llm-processor.service
```

Add this content (replace YOUR_USERNAME with your actual username):
```ini
[Unit]
Description=Techno-Notes
After=network.target

[Service]
Type=simple
User=YOUR_USERNAME
WorkingDirectory=/home/YOUR_USERNAME/Techno-Notes
ExecStart=/home/YOUR_USERNAME/Techno-Notes/venv/bin/python3 app.py
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Enable and start the service:
```bash
sudo systemctl enable pdf-llm-processor
sudo systemctl start pdf-llm-processor
sudo systemctl status pdf-llm-processor

# View logs
sudo journalctl -u pdf-llm-processor -f
```

### Logging Configuration

Logs are stored in `logs/app.log`. To adjust log level, edit `app.py`:
```python
logging.basicConfig(
    level=logging.DEBUG,  # Change to DEBUG for verbose logging
    # ...
)
```

## üìä API Endpoints

For developers who want to integrate programmatically:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Main web interface |
| `/setup_status` | GET | Get system setup status |
| `/auto_setup` | POST | Start automatic Ollama setup |
| `/process_pdf` | POST | Upload and process PDF |
| `/download/<file>` | GET | Download analysis result |
| `/get_prompt` | GET | Get current analysis prompt |
| `/save_prompt` | POST | Save custom analysis prompt |
| `/list_models` | GET | List available Ollama models |
| `/set_model` | POST | Change active Ollama model |
| `/get_provider` | GET | Get current LLM provider |
| `/set_provider` | POST | Set LLM provider (ollama/chatgpt/perplexity) |
| `/save_api_key` | POST | Save API key for cloud provider |
| `/get_api_key` | POST | Get saved API key for provider |
| `/test_api_connection` | POST | Test API connection |

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üìä Project Status

- ‚úÖ Core functionality complete
- ‚úÖ Security hardening applied
- ‚úÖ Documentation complete
- ‚úÖ Ready for production use
- üîÑ Ongoing maintenance and updates

## üí° Use Cases

### Business
- Contract analysis and review
- Report summarization
- Meeting minutes extraction
- Proposal evaluation

### Academic
- Research paper summarization
- Literature review assistance
- Thesis chapter analysis
- Citation extraction

### Legal
- Document discovery
- Contract clause identification
- Compliance checking
- Legal brief analysis

### Technical
- Technical documentation analysis
- API documentation extraction
- Code comment generation from specs
- Architecture decision records

## ‚ö° Performance Tips

1. **Choose the right model**: Smaller models (phi, llama3.2:1b) for quick analysis, larger models (llama2:13b) for quality
2. **Optimize PDFs**: Text-based PDFs process much faster than scanned documents
3. **Limit page count**: The tool processes up to 100 pages; for longer documents, split into sections
4. **Close other applications**: Free up RAM for better performance
5. **Use SSD storage**: Faster model loading and processing

## üîß Customization

### Custom Analysis Templates

Create specialized analysis templates by modifying the prompt. Examples:

**Financial Analysis:**
```
Analyze this financial document and extract:
- Revenue figures and trends
- Cost structures
- Profit margins
- Key financial ratios
- Risk factors mentioned
- Future projections
```

**Medical Records:**
```
Summarize this medical document including:
- Patient symptoms
- Diagnoses
- Treatment plans
- Medications prescribed
- Follow-up recommendations
- Critical findings
```

**Security Consultant Focused:**
```
Analyze this document from a security perspective:
- Identify security requirements
- List vulnerabilities or risks mentioned
- Extract compliance requirements
- Note security controls discussed
- Highlight security recommendations
- Flag any security gaps
```

## üåê Language Support

While the interface is in English, the LLM models support multiple languages for document analysis. You can customize prompts in different languages:

**Spanish:**
```
Analiza este documento PDF y proporciona un resumen completo...
```

**French:**
```
Analysez ce document PDF et fournissez un r√©sum√© complet...
```

**German:**
```
Analysieren Sie dieses PDF-Dokument und erstellen Sie eine umfassende Zusammenfassung...
```

## ‚≠ê Star History

If you find this project useful, please consider giving it a star on GitHub!
